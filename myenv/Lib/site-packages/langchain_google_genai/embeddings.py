import os
import random
import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI  # Google Generative AI for LLM
from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Correct import for embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Text splitting utility
from langchain.vectorstores import FAISS  # FAISS for vector storage
from langchain.document_loaders import PyPDFDirectoryLoader  # PDF loader
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
google_api_key = os.getenv('GOOGLE_API_KEY')

# Check if the Google API key is loaded correctly
if google_api_key is None:
    raise ValueError("Google API key not found. Please check your environment variables.")

# Initialize Google Generative AI LLM
llm = ChatGoogleGenerativeAI(api_key=google_api_key, model="gemini-1.5-pro", temperature=0.0, max_tokens=3000)

# Function to load and parse the PDF for Speaking Part 2
def create_vector_embedding_for_speaking_part2():
    try:
        # Initialize embeddings
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)

        # Check if the FAISS index already exists
        if os.path.exists("faiss_index"):
            # Load the FAISS index from the local path if it exists
            st.session_state.vectors = FAISS.load_local("faiss_index", embeddings)
        else:
            # If no FAISS index exists, create a new one
            # Load the directory where the PDF is saved
            st.session_state.loader = PyPDFDirectoryLoader("speaking2_pdf")
            
            # Document loading
            st.session_state.docs = st.session_state.loader.load()

            # Split the documents into smaller chunks for embeddings
            st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            st.session_state.final_documents = st.session_state.text_splitter.split_documents(st.session_state.docs)
            
            # Create FAISS vector store from documents and embeddings
            st.session_state.vectors = FAISS.from_documents(st.session_state.final_documents, embeddings)
            
            # Save the FAISS index locally for future use
            st.session_state.vectors.save_local("faiss_index")

    except Exception as e:
        st.write(f"Error initializing embeddings or FAISS index: {e}")
        return

# Function to extract questions from the document
def extract_speaking2_questions(documents):
    tasks = []

    for doc in documents:
        content = doc.page_content
        if not content:  # If there's no content, skip to the next document
            continue

        # Split the content into lines
        lines = content.split("\n")

        for line in lines:
            line = line.strip()
            # Check if the line starts with a number (e.g., "1. Describe an internet business...")
            if line and line[0].isdigit() and "." in line:
                # Extract the question
                parts = line.split(".", 1)  # Split on the first period
                if len(parts) == 2:
                    question = parts[1].strip()  # Get the question text
                    tasks.append({"question": question})  # Append it to the tasks list

    return tasks

# Function to generate similar questions using RAG (Retrieval-Augmented Generation)
def generate_similar_questions_using_rag():
    if "speaking2_tasks" in st.session_state and st.session_state.speaking2_tasks:
        tasks = st.session_state.speaking2_tasks
        # Pick a random task
        random_task = random.choice(tasks)
        question = random_task['question']

        # Initialize RAG retriever
        retriever = st.session_state.vectors.as_retriever()

        # Create a prompt for generating similar questions
        prompt = f"Based on the following question, generate similar questions:\n{question}"

        # Create a chain for RAG
        rag_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever
        )

        # Use RAG to generate new questions
        generated_questions = rag_chain.run(prompt)

        # Return both the original question and the generated questions
        return question, generated_questions
    else:
        return None, None  # No questions found or tasks are not yet ready.

# Function to display Speaking Part 2 content
def display_speaking2_content():
    st.title("IELTS Speaking Part 2 Question Generator with Google Generative AI")

    # Initialize embeddings and vector database on app start for Speaking Part 2
    if "speaking2_tasks" not in st.session_state:
        create_vector_embedding_for_speaking_part2()

    # Button to generate original question and similar ones
    if st.button("Generate Random Speaking Part 2 Question"):
        original_question, similar_questions = generate_similar_questions_using_rag()
        if original_question:
            st.session_state.original_question = original_question
            st.session_state.similar_questions = similar_questions
        else:
            st.write("No questions found or tasks are not yet ready.")
            
    # Display original question and similar ones
    if 'original_question' in st.session_state:
        st.write(f"**Original Question:** {st.session_state.original_question}")

        if 'similar_questions' in st.session_state:
            st.write("**Generated Similar Questions:**")
            st.write(st.session_state.similar_questions)

# Uncomment this part if you'd like to add transcription functionality later
# # Function to transcribe uploaded audio using Google Generative AI Whisper (or other models)
# def transcribe_audio(file):
#     if file:
#         with open(file.name, "rb") as audio_file:
#             output = openai.Audio.translate(
#                 model="whisper-1",   # Specify the model
#                 file=audio_file      # Provide the audio file
#             )
#         return output['text']  # The transcription result
#     return None

# Uncomment this if you'd like to add the ability to upload and transcribe audio responses
# # Audio file uploader for user to attach an audio response
# audio_file = st.file_uploader("Upload your audio response (MP3 format)", type=["mp3"])
# 
# # Transcribe audio when a file is uploaded
# if audio_file is not None:
#     transcription = transcribe_audio(audio_file)
#     if transcription:
#         st.write(f"**Transcription:** {transcription}")
#     else:
#         st.write("Failed to transcribe the audio. Please try again.")
